# Домашнее задание к занятию «Установка Kubernetes» - Леонид Хорошев

### Цель задания

Установить кластер K8s.

### Чеклист готовности к домашнему заданию

1. Развёрнутые ВМ с ОС Ubuntu 20.04-lts.


### Инструменты и дополнительные материалы, которые пригодятся для выполнения задания

1. [Инструкция по установке kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/).
2. [Документация kubespray](https://kubespray.io/).

-----

### Задание 1. Установить кластер k8s с 1 master node

1. Подготовка работы кластера из 5 нод: 1 мастер и 4 рабочие ноды.

Кластер готовим в Яндекс облаке. Наиболее оптимальным вариантом будет развернуть виртуальные машины через терраформ, чтобы в случае технических проблем (неправильный расчет вычислительных ресурсов или необходимость изменения количества нод) обеспечить возможность быстрых изменений.

Основной манифест [main.tf](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/hw-15/main.tf) описывает создание 5 виртуальных машин, из которых 4 имеют идентичные параметры (под worker ноды)
```tf
  network_interface {
    subnet_id = var.subnet_id
    nat       = var.nat
  }

  metadata = {
    user-data = "${file("/home/leo/kuber-homeworks/3.2/terraform/cloud-init.yaml")}"
 }
}

resource "yandex_compute_instance" "worker" {
  count           = var.worker_count
  name            = "worker-node-${count.index + 1}"
  platform_id     = var.worker_platform
  resources {
    cores         = var.worker_cores
    memory        = var.worker_memory
    core_fraction = var.worker_core_fraction
  }

  boot_disk {
    initialize_params {
      image_id = var.image_id
      size     = var.worker_disk_size
    }
  }

    scheduling_policy {
    preemptible = var.scheduling_policy
  }

  network_interface {
    subnet_id = var.subnet_id
    nat       = var.nat
  }

  metadata = {
    user-data = "${file("/home/leo/kuber-homeworks/3.2/terraform/cloud-init.yaml")}"
 }
}
```

Основные пользовательские параметры пропишем через в файле [cloud-init.yaml](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/hw-15/cloud-init.yaml). 
```yml
#cloud-config
users:
  - name: leo
    ssh_authorized_keys:
      - ssh-rsa 
    sudo: ['ALL=(ALL) NOPASSWD:ALL']
    groups: sudo
    shell: /bin/bash
package_update: true
package_upgrade: true
packages:
  - nginx
  - nano
  - software-properties-common
runcmd:
  - mkdir -p /home/leo/.ssh
  - chown -R leo:leo /home/leo/.ssh
  - chmod 700 /home/leo/.ssh
  - sudo add-apt-repository ppa:deadsnakes/ppa -y
  - sudo apt-get update
```
Далее привожу ссылки на прочие файлы, необходимые для корректной работы манифеста:

[variables.tf](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/hw-15/variables.tf)

[providers.tf](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/hw-15/providers.tf)

Поднимаем инфраструктуру
```
terraform init
terraform plan
terraform apply
```
![Alt_text](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/main/3.2/screenshots/k8s1.png)

Cкачиваем репозитрий с kubespray
```
git clone https://github.com/kubernetes-sigs/kubespray
```
Устанавливаем зависимости
```
sudo pip3 install -r requirements.txt
```
Копируем шаблон
```
cp -rfp inventory/sample inventory/mycluster
```

2. В качестве CRI — containerd.

3. Запуск etcd производить на мастере.

CRI и ноду с etcd прописываем в файле [inventory.ini](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/hw-15/inventory.ini) (файл отредактирован в ручном режиме).
```yml
# ## Configure 'ip' variable to bind kubernetes services on a
# ## different ip than the default iface
# ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value.
[all]
node1 ansible_host=84.201.175.172 # ip=10.128.0.33 etcd_member_name=etcd1
node2 ansible_host=51.250.88.225  # ip=10.128.0.14 etcd_member_name=etcd2
node3 ansible_host=178.154.204.135 # ip=10.128.0.16 etcd_member_name=etcd3
node4 ansible_host=178.154.207.97 # ip=10.128.0.36 etcd_member_name=etcd4
node5 ansible_host=158.160.50.16  # ip=10.128.0.4 etcd_member_name=etcd5
# node6 ansible_host=95.54.0.17  # ip=10.3.0.6 etcd_member_name=etcd6
[all:vars]
ansible_python_interpreter=/usr/bin/python3
# ## configure a bastion host if your nodes are not directly reachable
# [bastion]
# bastion ansible_host=x.x.x.x ansible_user=some_user

[kube_control_plane]
node1

[kube_master]
node1
# node2
# node3

[etcd]
node1
# node2
# node3

[kube_node]
node2
node3
node4
node5
# node6

[calico_rr]

[k8s_cluster:children]
kube_master
kube_node
calico_rr
```
Теперь нужно проверить, что kubespray настроен для использования containerd в качестве CRI.
`k8s-cluster.yml`
```
kubelet_container_runtime: containerd
container_manager: containerd
```

4. Способ установки выбрать самостоятельно.

Устанавливаем кластер через `ansible`
```
ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml -b -v
```
![Alt_text](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/main/3.2/screenshots/k8s2.png)

Проверяем, что кластер работает корректно
```
ssh leo@84.201.175.172
sudo kubectl get nodes
```
![Alt_text](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/main/3.2/screenshots/k8s3.png)

Примечание: в файлах [providers.tf](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/hw-15/providers.tf) и  [main.tf](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/hw-15/main.tf) имеются закомментированные блоки кода по созданию файла, который предполагалось использовать как inventory для kubespray, но почему-то его kubespray воспринимал некорректно, поэтому было принято решение вручную прописать ip адреса в прилагаемый к репозиторию файл [inventory.ini](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/hw-15/inventory.ini).

## Дополнительные задания (со звёздочкой)

**Настоятельно рекомендуем выполнять все задания под звёздочкой.** Их выполнение поможет глубже разобраться в материале.   
Задания под звёздочкой необязательные к выполнению и не повлияют на получение зачёта по этому домашнему заданию. 

------
### Задание 2*. Установить HA кластер

1. Установить кластер в режиме HA.
2. Использовать нечётное количество Master-node.

Для увеличения количества control-plane нод в манифесте из предыдущего задания изменен первый блок. 
```tf
resource "yandex_compute_instance" "control-plane" {
  count           = var.control_plane_count
  name            = "control-plane-${count.index + 1}"
  platform_id     = var.platform
  resources {
    cores         = var.control_plane_core
    memory        = var.control_plane_memory
    core_fraction = var.control_plane_core_fraction
  }
```
Полный код манифеста прилагаю [main.tf](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/hw-15/ha-main.tf)

Также в `variables.tf` добавлена переменная
```
variable "control_plane_count" {
  type        = number
  default     = 3
```

Применяем изменения
```
terraform apply
```

![Alt_text](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/main/3.2/screenshots/k8s4.png)

3. Для cluster ip использовать keepalived или другой способ.

Аналогично с заданием 1 редактируем файл [inventory.ini](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/hw-15/ha-inventory.ini)
```
[all]
node1 ansible_host=178.154.222.141 # ip=10.128.0.23  etcd_member_name=etcd1
node2 ansible_host=51.250.70.224  # ip=10.128.0.35 etcd_member_name=etcd2
node3 ansible_host=178.154.204.243 # ip=10.128.0.14 etcd_member_name=etcd3
node4 ansible_host=158.160.101.168 # ip=10.128.0.21 etcd_member_name=etcd4
node5 ansible_host=62.84.126.224  # ip=10.128.0.8 etcd_member_name=etcd5
node6 ansible_host=51.250.93.241  # ip=10.128.0.27 etcd_member_name=etcd5
node7 ansible_host=178.154.220.99  # ip=10.128.0.29 etcd_member_name=etcd5

# node6 ansible_host=95.54.0.17  # ip=10.3.0.6 etcd_member_name=etcd6
[all:vars]
ansible_python_interpreter=/usr/bin/python3
# ## configure a bastion host if your nodes are not directly reachable
# [bastion]
# bastion ansible_host=x.x.x.x ansible_user=some_user

[kube_control_plane]
node1
node2
node3

[kube_master]
node1
node2
node3

[etcd]
node1
node2
node3

[kube_node]
node4
node5
node6
node7
# node6

[calico_rr]

[k8s_cluster:children]
kube_master
kube_node
```

Для настроек keepalived отредактируем файл [inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/hw-15/ha-k8s-cluster.yml), добавив ткда следующий раздел
```
# Enable loadbalancer for kubeapi server
loadbalancer_apiserver:
  address: 10.128.0.1  
  port: 6443

keepalived_vip:
  enabled: true
  interface: eth0 
  virtual_router_id: 51
  auth_type: PASS
  auth_pass: 42
  nodes:
    - name: node1
      ip: 178.154.222.141
      priority: 101  # Приоритет для узла node1
    - name: node2
      ip: 51.250.70.224
      priority: 100  # Приоритет для узла node2
    - name: node3
      ip: 178.154.204.243
      priority: 99   # Приоритет для узла node3
```

- `10.128.0.1` - виртуальный IP-адрес, который будет использован для доступа к API серверу
- `eth0`   - сетевой интерфейс, используемый Keepalived
- `priority` - приоритеты трех control-plane нод.

Сохраняем изменения и запускаем наш плейбук
```
ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml -b -v
```

Плейбук запускался несколько раз, но до конца ни разу не отработал. Проблема судя по всему во взаимодействии etcd на разных нодах, но решить ее не удалось
```
FAILED - RETRYING: [node1]: Create kubeadm token for joining nodes with 24h expiration (default) (5 retries left).
FAILED - RETRYING: [node2 -> node1]: Create kubeadm token for joining nodes with 24h expiration (default) (5 retries left).
FAILED - RETRYING: [node3 -> node1]: Create kubeadm token for joining nodes with 24h expiration (default) (5 retries left).
```
![Alt_text](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/main/3.2/screenshots/k8s5.png)



### Правила приёма работы

1. Домашняя работа оформляется в своем Git-репозитории в файле README.md. Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.
2. Файл README.md должен содержать скриншоты вывода необходимых команд `kubectl get nodes`, а также скриншоты результатов.
3. Репозиторий должен содержать тексты манифестов или ссылки на них в файле README.md.
