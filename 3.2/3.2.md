# Домашнее задание к занятию «Установка Kubernetes»

### Цель задания

Установить кластер K8s.

### Чеклист готовности к домашнему заданию

1. Развёрнутые ВМ с ОС Ubuntu 20.04-lts.


### Инструменты и дополнительные материалы, которые пригодятся для выполнения задания

1. [Инструкция по установке kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/).
2. [Документация kubespray](https://kubespray.io/).

-----

### Задание 1. Установить кластер k8s с 1 master node

1. Подготовка работы кластера из 5 нод: 1 мастер и 4 рабочие ноды.

Кластер готовим в Яндекс облаке. Наиболее оптимальным вариантом будет развернуть виртуальные машины через терраформ, чтобы в случае технических проблем (неправильный расчет вычислительных ресурсов или необходимость изменения количества нод) обеспечить возможность быстрых изменений.

Основной манифест `main.tf` описывает создание 5 виртуальных машин, из которых 4 имеют идентичные параметры (под worker ноды)
```tf
resource "yandex_compute_instance" "control-plane" {
  name            = var.control_plane_name
  platform_id     = var.platform
  resources {
    cores         = var.control_plane_core
    memory        = var.control_plane_memory
    core_fraction = var.control_plane_core_fraction
  }

  boot_disk {
    initialize_params {
      image_id = var.image_id
      size     = var.control_plane_disk_size
    }
  }

  scheduling_policy {
    preemptible = var.scheduling_policy
  }

  network_interface {
    subnet_id = var.subnet_id
    nat       = var.nat
  }

  metadata = {
    user-data = "${file("/home/leo/kuber-homeworks/3.2/terraform/cloud-init-control-plane.yaml")}"
 }
}

resource "yandex_compute_instance" "worker" {
  count           = var.worker_count
  name            = "worker-node-${count.index + 1}"
  platform_id     = var.worker_platform
  resources {
    cores         = var.worker_cores
    memory        = var.worker_memory
    core_fraction = var.worker_core_fraction
  }

  boot_disk {
    initialize_params {
      image_id = var.image_id
      size     = var.worker_disk_size
    }
  }

    scheduling_policy {
    preemptible = var.scheduling_policy
  }

  network_interface {
    subnet_id = var.subnet_id
    nat       = var.nat
  }
  metadata = {
    user-data = "${file("/home/leo/kuber-homeworks/3.2/terraform/cloud-init-worker.yaml")}"
 }
}
```

Основные пользовательские параметры пропишем через в файле `cloud-init.yaml`. Так как для `contro-plane` и `worker` требуются разные пакеты, то целесообразно подготовить отдельные файлы для каждого типа нод.

`cloud-init-control-plane.yaml`
```yml
#cloud-config
users:
  - default
  - name: leo
    ssh_authorized_keys:
      - ssh-rsa 
    sudo: ['ALL=(ALL) NOPASSWD:ALL']
    groups: sudo
    shell: /bin/bash
packages:
  - nginx
  - nano
  - git
package_update: true
runcmd:
  - apt-get update
  - apt-get install -y software-properties-common
  - add-apt-repository -y ppa:deadsnakes/ppa
  - apt-get update
  - apt-get install -y python3.11 python3.11-venv python3.11-dev
  - apt-get install -y python3-pip
  - pip3 install --upgrade pip
  - pip3 install ansible>=2.14.0
```

`cloud-init-control-plane.yaml`
```yml
#cloud-config
users:
  - default
  - name: leo
    ssh_authorized_keys:
      - ssh-rsa 
    sudo: ['ALL=(ALL) NOPASSWD:ALL']
    groups: sudo
    shell: /bin/bash
packages:
  - nginx
  - nano
package_update: true
runcmd:
  - apt-get update
  - apt-get install -y software-properties-common
  - add-apt-repository -y ppa:deadsnakes/ppa
  - apt-get update
  - apt-get install -y python3.11 python3.11-venv python3.11-dev
  - apt-get install -y python3-pip
```
Далее привожу ссылки на прочие файлы, необходимые для корректной работы манифеста:

[variables.tf]()

[providers.tf]()

Поднимаем инфраструктуру
```
terraform init
terraform plan
terraform apply
```
![Alt_text]()

Переходим на control-plane ноду и скачиваем репозитрий с kubespray
```
ssh leo@178.154.228.102
git clone https://github.com/kubernetes-sigs/kubespray
```

2. В качестве CRI — containerd.
3. Запуск etcd производить на мастере.

CRI и ноду с etcd прописываем в файле `inventory.ini`.
```yml
# ## Configure 'ip' variable to bind kubernetes services on a
# ## different ip than the default iface
# ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value.
[all]
node1 ansible_host=178.154.228.102 ip=10.128.0.18 etcd_member_name=etcd1
node2 ansible_host=178.154.231.243 ip=10.128.0.17 etcd_member_name=etcd2
node3 ansible_host=178.154.225.64  ip=10.128.0.27 etcd_member_name=etcd3
node4 ansible_host=178.154.228.130 ip=10.128.0.20 etcd_member_name=etcd4
node5 ansible_host=51.250.11.73    ip=10.128.0.22 etcd_member_name=etcd5
# node6 ansible_host=95.54.0.17  # ip=10.3.0.6 etcd_member_name=etcd6

# ## configure a bastion host if your nodes are not directly reachable
# [bastion]
# bastion ansible_host=x.x.x.x ansible_user=some_user

[kube_control_plane]
node1
# node2
# node3

[etcd]
node1
# node2
# node3

[kube_node]
node2
node3
node4
node5
# node6

[calico_rr]

[k8s_cluster:children]
kube_control_plane
kube_node
calico_rr

[containerd]
node1
node2
node3
node4
node5
```
4. Способ установки выбрать самостоятельно.

## Дополнительные задания (со звёздочкой)

**Настоятельно рекомендуем выполнять все задания под звёздочкой.** Их выполнение поможет глубже разобраться в материале.   
Задания под звёздочкой необязательные к выполнению и не повлияют на получение зачёта по этому домашнему заданию. 

------
### Задание 2*. Установить HA кластер

1. Установить кластер в режиме HA.
2. Использовать нечётное количество Master-node.
3. Для cluster ip использовать keepalived или другой способ.

### Правила приёма работы

1. Домашняя работа оформляется в своем Git-репозитории в файле README.md. Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.
2. Файл README.md должен содержать скриншоты вывода необходимых команд `kubectl get nodes`, а также скриншоты результатов.
3. Репозиторий должен содержать тексты манифестов или ссылки на них в файле README.md.
