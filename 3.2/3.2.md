# Домашнее задание к занятию «Установка Kubernetes»

### Цель задания

Установить кластер K8s.

### Чеклист готовности к домашнему заданию

1. Развёрнутые ВМ с ОС Ubuntu 20.04-lts.


### Инструменты и дополнительные материалы, которые пригодятся для выполнения задания

1. [Инструкция по установке kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/).
2. [Документация kubespray](https://kubespray.io/).

-----

### Задание 1. Установить кластер k8s с 1 master node

1. Подготовка работы кластера из 5 нод: 1 мастер и 4 рабочие ноды.

Кластер готовим в Яндекс облаке. Наиболее оптимальным вариантом будет развернуть виртуальные машины через терраформ, чтобы в случае технических проблем (неправильный расчет вычислительных ресурсов или необходимость изменения количества нод) обеспечить возможность быстрых изменений.

Основной манифест `main.tf` описывает создание 5 виртуальных машин, из которых 4 имеют идентичные параметры (под worker ноды)
```tf
  network_interface {
    subnet_id = var.subnet_id
    nat       = var.nat
  }

  metadata = {
    user-data = "${file("/home/leo/kuber-homeworks/3.2/terraform/cloud-init.yaml")}"
 }
}

resource "yandex_compute_instance" "worker" {
  count           = var.worker_count
  name            = "worker-node-${count.index + 1}"
  platform_id     = var.worker_platform
  resources {
    cores         = var.worker_cores
    memory        = var.worker_memory
    core_fraction = var.worker_core_fraction
  }

  boot_disk {
    initialize_params {
      image_id = var.image_id
      size     = var.worker_disk_size
    }
  }

    scheduling_policy {
    preemptible = var.scheduling_policy
  }

  network_interface {
    subnet_id = var.subnet_id
    nat       = var.nat
  }

  metadata = {
    user-data = "${file("/home/leo/kuber-homeworks/3.2/terraform/cloud-init.yaml")}"
 }
}
```

Основные пользовательские параметры пропишем через в файле `cloud-init.yaml`. 
`cloud-init-control-plane.yaml`
```yml
#cloud-config
users:
  - name: leo
    ssh_authorized_keys:
      - ssh-rsa 
    sudo: ['ALL=(ALL) NOPASSWD:ALL']
    groups: sudo
    shell: /bin/bash
package_update: true
package_upgrade: true
packages:
  - nginx
  - nano
  - software-properties-common
runcmd:
  - mkdir -p /home/leo/.ssh
  - chown -R leo:leo /home/leo/.ssh
  - chmod 700 /home/leo/.ssh
  - sudo add-apt-repository ppa:deadsnakes/ppa -y
  - sudo apt-get update
```
Далее привожу ссылки на прочие файлы, необходимые для корректной работы манифеста:

[variables.tf]()

[providers.tf]()

Поднимаем инфраструктуру
```
terraform init
terraform plan
terraform apply
```
![Alt_text](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/main/3.2/screenshots/k8s1.png)

Cкачиваем репозитрий с kubespray
```
git clone https://github.com/kubernetes-sigs/kubespray
```
Устанавливаем зависимости
```
sudo pip3 install -r requirements.txt
```
Копируем шаблон
```
cp -rfp inventory/sample inventory/mycluster
```

2. В качестве CRI — containerd.

3. Запуск etcd производить на мастере.

CRI и ноду с etcd прописываем в файле `inventory.ini` (файл отредактирован в ручном режиме).
```yml
# ## Configure 'ip' variable to bind kubernetes services on a
# ## different ip than the default iface
# ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value.
[all]
node1 ansible_host=84.201.175.172 # ip=10.128.0.33 etcd_member_name=etcd1
node2 ansible_host=51.250.88.225  # ip=10.128.0.14 etcd_member_name=etcd2
node3 ansible_host=178.154.204.135 # ip=10.128.0.16 etcd_member_name=etcd3
node4 ansible_host=178.154.207.97 # ip=10.128.0.36 etcd_member_name=etcd4
node5 ansible_host=158.160.50.16  # ip=10.128.0.4 etcd_member_name=etcd5
# node6 ansible_host=95.54.0.17  # ip=10.3.0.6 etcd_member_name=etcd6
[all:vars]
ansible_python_interpreter=/usr/bin/python3
# ## configure a bastion host if your nodes are not directly reachable
# [bastion]
# bastion ansible_host=x.x.x.x ansible_user=some_user

[kube_control_plane]
node1

[kube_master]
node1
# node2
# node3

[etcd]
node1
# node2
# node3

[kube_node]
node2
node3
node4
node5
# node6

[calico_rr]

[k8s_cluster:children]
kube_master
kube_node
calico_rr
```
Теперь нужно проверить, что kubespray настроен для использования containerd в качестве CRI.
`k8s-cluster.yml`
```
kubelet_container_runtime: containerd
container_manager: containerd
```

4. Способ установки выбрать самостоятельно.

Устанавливаем кластер через `ansible`
```
ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml -b -v
```
![Alt_text](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/main/3.2/screenshots/k8s2.png)

Проверяем, что кластер работает корректно
```
ssh leo@84.201.175.172
sudo kubectl get nodes
```
![Alt_text](https://github.com/LeonidKhoroshev/kuber-homeworks/blob/main/3.2/screenshots/k8s3.png)




## Дополнительные задания (со звёздочкой)

**Настоятельно рекомендуем выполнять все задания под звёздочкой.** Их выполнение поможет глубже разобраться в материале.   
Задания под звёздочкой необязательные к выполнению и не повлияют на получение зачёта по этому домашнему заданию. 

------
### Задание 2*. Установить HA кластер

1. Установить кластер в режиме HA.
2. Использовать нечётное количество Master-node.
3. Для cluster ip использовать keepalived или другой способ.

### Правила приёма работы

1. Домашняя работа оформляется в своем Git-репозитории в файле README.md. Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.
2. Файл README.md должен содержать скриншоты вывода необходимых команд `kubectl get nodes`, а также скриншоты результатов.
3. Репозиторий должен содержать тексты манифестов или ссылки на них в файле README.md.
